documentclass[11pt]{article}

\begin{document}

\title{The cost of cloud computing for {\em de novo} transcriptome assembly on cloud computers}
\author{Leigh Sheneman and C. Titus Brown}
\maketitle

\section*{Abstract}

\section*{Introduction}

Within biology, the number and size of sequencing data sets is
increasing rapidly, leading to an increased demand for computational
infrastructure. Many researchers and institutions are finding it
difficult to cope with this increased demand.  One possible solution
is to rent compute power on demand from a commercial provider -- so
called ``cloud computing.'' Cloud computing has been proposed as an
alternative to purchasing and  maintaining dedicated servers with
technology that quickly becomes outdated.

Cloud computing offers many advantages both for individual researchers
and for teaching workshops; cloud providers such as Amazon Web
Services, Microsoft Azure, Google Compute Engine, and Rackspace Cloud
can scale with hourly demand and provide a range of hardware and
software configurations.  Moreover, cloud computing offers an
opportunity for reproducibility in computational biology, because the
exact execution environment behind a paper can be made available to
readers.

However, cloud computing resources can also be a poor fit for
computational research.  Cloud computing relies on virtualization
technology to provide a configurable execution environment to users;
this yields poorer computational performance than execution on non-
virtualized hardware.  Moreover, cloud computing providers do not
typically have specialized hardware or computers with substantial
amounts of random-access memory (RAM); for example, Google Compute
Engine currently limits computers to 104 gigabytes (GB) of RAM. And
finally, cloud computing costs can be unpredictable: because charges
are based on usage, processing tasks that take longer cost more, and
the length of a task cannot always be predicted in advance.

Below we evaluate the computational requirements and overall cost of a
data- and memory-intensive work-flow for {\em de novo} reconstruction
of transcriptomes from Illumina sequencing reads, running on a
published {\em Nematostella} data set.  We show that different steps
in the work-flow have dramatically different computational
requirements from one another, and that substantial differences in the
time to run the work-flow can be achieved by choosing different
providers or hardware. We also compare and contrast the cost of
sequencing with the cost of the associated computation.

\section*{Results}

Our sequence analysis work-flow has multiple steps, including primer
trimming, quality filtering, data preprocessing with digital
normalization, primary data processing in the form of assembly,
annotation against existing databases, and correlation of the assembly
results with the original data via mapping.  Each of these steps has
its own computational requirements: primer trimming and quality
filtering are primarily CPU- and disk-intensive, while digital
normalization and assembly are memory intensive, annotation is CPU-
intensive, and mapping is both CPU- and disk- intensive.  These are
unavoidable given the underlying algorithms.

(@LS, how can we best measure io wait? @CTB vmstat -- time spent
waiting between recursive calls;  sar --  \%wio from sar -u -> i/o
bound time;   avwait to avserv form sar -d will show bottleneck;
(user + nice + system) cpu usage)

Transcriptome assembly relies on several memory intensive steps, with
only sporadic intensive disk use and relatively low levels of CPU
usage compared to a mapping work-flow. The memory requirements for the
Trinity assembler are hard to predict a priori because they depend on
biological features of the data -- here, the underlying information
content of the transcriptome being sequenced, together with the errors
remaining after digital normalization.  This is a common feature of De
Bruijn graph assemblers, whose memory use scales with the sum of
information content and erroneous sequence.  In practice this means
that large transcriptomes or transcriptomes from multiple species may
require substantially more memory to assemble than small, single-
species transcriptomes.

The cost of computation for these data sets is approximately \$100 for
each of the configurations (see table XX), with lower costs typically
incurring higher overall compute time and delaying delivery of
results. Regardless, the cost of computation for the data sets is
dwarfed by the expense of generating the data: \$1000 of data needs
about \$100 of computing.

While each provider has a wealth of resources for virtual machines to
draw  from, they present them in pre-configured packages. One cannot
simply switch to a larger hard drive, you must upgrade your entire set
of components to  increase a single factor. For example, AWS has 22 of
these ``tiers'' in the  current generation of hardware alone. Their
prices range from \$0.02 to \$6.82 per hour, thus proper resource
estimation is vital for users with limited fund availability.

A minimal requirement is to be able to provide enough disk space for a
data set to run through the protocol with out depletion. Even the
Nematostella data  we used needs 200GB to complete the pipeline, thus
ruling out the exclusively using a m1.xlarge instance.

Though each provider has similar available resources, the hardware
implementation of the cluster is a factor in performance. When renting
from Rackspace the resources used by your virtual machine are in the
same rack. On the other hand AWSâ€™s instances are connected over the
network. Intuitively, when all the components are in close proximity
you experience higher input rates than if the data has to travel
through 20 feet of networking cable. It  also stands to reason, if our
algorithms and data structures are encapsulated  in the the CPU cache
the performance will improve an order of magnitude.

\section*{Discussion}

It is difficult predict the performance of our full de novo
transcriptome assembly work-flow on a new computer hardware
configuration, given the many different components, each with their
own distinct computational requirements.

Optimizing work-flows for multiple cloud providers is therefore
inherently challenging.  In addition to different base configurations
of disk and CPU performance and memory size, cloud providers may be
running virtual machines on heterogeneous hardware, and sharing disk
and network bandwidth with other applications.  Each cloud provider
will also have a distinct set of costs associated with each of their
hardware configurations (e.g. Amazon, network, EBS), with non-obvious
implications for the work-flow.  And each cloud provider typically
makes available multiple configurations of CPU, disk, and memory, each
of which will have its own performance characteristics.  This makes
choosing the optimum computing platform a difficult multidimensional
optimization problem.

By establishing the limiting factor for each step of our assembly
protocol  we have discovered bounds for platform selection for that
step. With the help of virtualization, it is easy to switch between
configurations at each  juncture. Thus, we can reduce the complexity
of the optimization problem by  focusing on the particular computation
needs at each step. In addition, if it is found the data reaches a
bottleneck a few steps in, we can reconfigure the setup and proceed
with a minimal loss of time. This process can result in a  substantial
reduction of cost, since we can make an educated guess what the  most
fitting CPU, disk, and memory requirements will be then adjust to a
larger machine if need be.

Discuss ``surfacing'' otherwise hidden costs from HPCs.

(@CTB I assume you are referring to maintenance, energy cost, etc?)

\end{document}
